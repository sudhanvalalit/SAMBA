{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#samba-package-sandbox-for-mixing-using-bayesian-analysis","title":"SAMBA package: SAndbox for Mixing using Bayesian Analysis","text":"<p>Bayesian model mixing is a novel concept that surpasses the more widely used Bayesian model averaging (BMA) in its use of location-dependent weights, allowing multiple models to dominate in regions where they are the most accurate model known, instead of averaging over all of the models in the space. In this repo, we store the preliminary code and tests of three model mixing techniques studied on a test case meant to symbolise the type of nuclear physics problem we want to apply these methods to in the future. This will, in the next few months, be compiled into a package to be released this spring/summer (2022) under the name of SAMBA---SAndbox for Mixing using Bayesian Analysis. This accompanies the paper we have posted to arXiv (see the paper here) where we detail our work using these techniques on our toy problem: the zero-dimensional $\\phi^4$ theory partition function.</p>"},{"location":"#about","title":"About","text":"<p>This package is intended to contain a variety of methods to perform Bayesian Model Mixing on a given set of data points and errors across an input space. In the current version, it can apply three different BMM methods to series expansions in the limits of a coupling constant. In future versions, we will include the option for users to implement their own functions or data sets to mix using SAMBA. It is a part of the BAND collaboration's v0.2 software. </p> <p></p>"},{"location":"#workflow","title":"Workflow","text":"<ul> <li> <p>Instantiate an object using the method of your choice (<code>LMM</code> (linear mixture model), <code>Bivariate</code> (bivariate BMM), or <code>GP</code> (trivariate BMM with a GP)) and select the truncation orders for the small-g and large-g expansions. </p> </li> <li> <p>Mix the models using the provided functions in the method classes listed above. </p> </li> </ul> <p>Refer to the Tutorials folder in the repo for a comprehensive set of notebooks on how to implement SAMBA on the current toy model setup. </p> <p>An animated <code>.gif</code> of our toy models is below. </p> <p> </p>"},{"location":"#testing","title":"Testing","text":"<p>At its present status, there are two ways to test SAMBA:</p> <ul> <li> <p>After cloning this repo, make sure you have all of the required packages installed (see requirements.txt for a list of external packages needed) and open the docs/Tutorials folder. Open the 3 notebooks there and run each notebook. If each cell is free of any errors, SAMBA is working correctly!</p> </li> <li> <p>After cloning this repo, open a terminal and go to the Tests folder. This contains two pytest-compatible files (stay tuned for more!) that test the Bivariate and GP methods. Type <code>pytest</code> in your terminal and it should run both of the files. If all tests are passing and only a couple of warnings (about external packages) show up, SAMBA is passing its tests!</p> </li> </ul>"},{"location":"discrepancy/","title":"Method 2: Bivariate model mixing","text":"<p>In this method, we use pointwise bivariate model mixing, or precision-weighted mixing, requiring models to be evaluated at every point in the input space where we desire a prediction to be made. This method can be written succinctly as</p> <p>$$ f_{\\dagger} = \\frac{1}{Z_P}\\sum_{k=1}^{K} \\frac{1}{v_k}f_k,    \\qquad Z_P \\equiv \\sum_{k=1}^{K}\\frac{1}{v_k}, $$</p> <p>where we can also define</p> <p>$$ f_{\\dagger} \\sim \\mathcal{N}\\bigl(Z_P^{-1}\\sum_k \\frac{1}{v_k}f_k, Z_P^{-1}\\bigr). $$</p> <p>This method is precision-weighted because it uses the variances of the models at each point in the input space as the inverse weights of the corresponding model prediction, hence the model with the smallest variance at a given point will dominate the mixed model at that location.</p> <p>:: samba.discrepancy</p>"},{"location":"fprdat/","title":"Fractional Power of Rational function method (FPR) data","text":"<p>This class contains the data for the results of the FPR method used in Honda. It obtains a very precise mean function, but is unable to generate the uncertainty bands that Bayesian model mixing can achieve. It is used  as a comparison in the paper published with this package.</p> <p>:: samba.fprdat</p>"},{"location":"gaussianprocess/","title":"Method 3: Multivariate model mixing with a Gaussian process","text":"<p>This method uses the same framework as the previous method, but now includes a Gaussian process (GP) in the mixing.</p> <p>A diagnostic tool that helps with determining whether or not our mixed model result is reasonable is the Mahalanobis distance, calculated as</p> <p>$$ D^{2}_{MD} = (\\mathbf{y} - \\mathbf{m})^{T}\\textit{K}^{-1}(\\mathbf{y} - \\mathbf{m}), $$</p> <p>and given in the functions below.</p> <p>:: samba.gaussprocess</p>"},{"location":"mixing/","title":"Method 1: Linear model mixing","text":"<p>This method is derived from Coleman's thesis, and  uses a mixing function with hyperparameters to be estimated using data, to construct the mixed model. There are several possible mixing functions given in the code below for a user to play with and build off of to write their own mixing function. There are also priors to choose from (in the <code>priors.py</code> file) for the hyperparameters of each mixing function.</p> <p>Once the mixing function has been chosen, and data supplied or simulated, the user can construct the mixed model by sampling the parameter space using the sampler wrapper below, and then building the posterior predictive distribution (PPD). This is given as</p> <p>$$ p(\\tilde y(g)|\\theta, \\mathbf{D}) = \\sum_{j=1}^{M} \\alpha(g; \\theta_{j}) F^{N_s}(g) + (1 - \\alpha(g; \\theta_{j})) F^{N_l}_{l}(g), $$</p> <p>where $\\alpha(g; \\theta_{j})$ is the chosen mixing function with hyperparameters $\\theta_{j}$.</p> <p>:: samba.mixing</p>"},{"location":"models/","title":"Models","text":"<p>The models in SAMBA are two expansions of a toy model, where the full toy model is given by</p> <p>$$  F(g) = \\int_{-\\infty}^{\\infty} dx~ e^{-\\frac{x^{2}}{2} - g^{2} x^{4}} = \\frac{e^{\\frac{1}{32 g^{2}}}}{2 \\sqrt{2}g} K_{\\frac{1}{4}}\\left(\\frac{1}{32 g^{2}} \\right). $$</p> <p>The two expansions are limits taken at $g = 0$ and $g = \\infty$:</p> <p>$$ F_{s}^{N_s}(g) = \\sum_{k=0}^{N_{s}} s_{k} g^{k}, $$</p> <p>and </p> <p>$$ F_{l}^{N_{l}}(g) = \\frac{1}{\\sqrt{g}} \\sum_{k=0}^{N_{l}} l_{k} g^{-k}, $$</p> <p>with coefficients given as:</p> <p>$$ s_{2k} = \\frac{\\sqrt{2} \\Gamma{(2k + 1/2)}}{k!} (-4)^{k},~~~~~s_{2k + 1} = 0 $$</p> <p>and</p> <p>$$ l_{k} = \\frac{\\Gamma{\\left(\\frac{k}{2} + \\frac{1}{4}\\right)}}{2k!} \\left(-\\frac{1}{2}\\right)^{k}. $$</p> <p>We also include models for the uncertainties of each expansion, given in the uninformative limit, for the small-$g$ expansion, as</p> <p>$$ \\sigma_{N_s}(g)= \\Gamma(N_s+3) g^{N_s + 2} \\bar{c}, $$</p> <p>if $N_s$ is even, and</p> <p>$$ \\sigma_{N_s}(g)= \\Gamma(N_s+2) g^{N_s+1} \\bar{c}, $$</p> <p>if $N_s$ is odd. For the large-$g$ limit,</p> <p>$$ \\sigma_{N_l}(g)=\\frac{1}{\\Gamma(N_l+2)} \\frac{1}{g^{N_l+3/2}} \\bar{d}. $$</p> <p>We also devise expressions for the informative limit, for the small-$g$ expansion, as</p> <p>$$ \\sigma_{N_s}(g)= \\Gamma(N_s/2+1) (4g)^{N_s + 2} \\bar{c},  $$</p> <p>if $N_s$ is even, and</p> <p>$$ \\sigma_{N_s}(g)= \\Gamma(N_s/2+1/2) (4g)^{N_s+1} \\bar{c}, $$</p> <p>if $N_s$ is odd. For the large-$g$ limit,</p> <p>$$ \\sigma_{N_l}(g)=\\left(\\frac{1}{4g}\\right)^{N_l + 3/2} \\frac{1}{\\Gamma(N_l/2+3/2)} \\bar{d}. $$</p> <p>:: samba.models</p>"},{"location":"priors/","title":"Priors for the linear model mixing hyperparameter estimation","text":"<p>This class contains the priors developed for the hyperparameters of the mixing function for the first method, linear model mixing.  This class can be expanded by the user to employ any other desired priors.</p> <p>:: samba.priors</p>"},{"location":"Tutorials/","title":"SAMBA Tutorials","text":"<p>Welcome to the tutorials designed to help you navigate SAMBA! There are three tutorial Jupyter notebooks contained in this folder to guide you through the three different BMM methods in SAMBA: the Linear Mixture Model (LMM.ipynb), Bivariate BMM (Bivariate_BMM.ipynb), and Trivariate BMM with a GP (GP_BMM.ipynb). Each tutorial has benchmark values you can check against to make sure SAMBA is operating properly on your machine. Open any of the notebooks to get started and see how SAMBA employs BMM with the current toy model setup! If you would like to see a walkthrough of the toy model problem before getting to the code, open <code>LMM.ipynb</code> first, where it has been laid out in full. </p> <p>If you want to explore even more BMM methods, check out the other BMM BAND Collaboraton package, Taweret!</p>"},{"location":"Tutorials/Bivariate_BMM/","title":"An introduction to SAMBA: Bivariate Bayesian Model Mixing","text":"<p>From the last notebook, you saw the linear mixture model in action. Now we are going to look at the second method in the paper: Bivariate Bayesian Model Mixing. This is the easiest method to implement in code. We are simply going to take our two series expansions, $F_{s}^{N_{s}}(g)$ and $F_{l}^{N_{l}}(g)$, and mix them according to $$ f^{\\dagger} \\sim \\bigl(Z_P^{-1}\\sum_k \\frac{1}{v_k}f_k, Z_P^{-1}\\bigr), $$</p> <p>where</p> <p>$$ Z_P \\equiv \\sum_{k=1}^{K}\\frac{1}{v_k}. $$</p> <p>$Z_P$ is the precision, or the inverse of the variance, of the $K$ models, $v_{k}$ the individual variances of each model (which we previously denoted the theory error), and $f^{\\dagger}$ the mixed model.</p> <p>We start by loading all of our necessary packages and classes.</p> In\u00a0[2]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\n#import packages\nimport numpy as np\nimport math\nimport statistics\nfrom scipy import stats, special, integrate\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import AutoMinorLocator\n%matplotlib inline\n\n#matplotlib settings for Latex plots\nimport matplotlib\n</pre> %load_ext autoreload %autoreload 2  #import packages import numpy as np import math import statistics from scipy import stats, special, integrate  import matplotlib.pyplot as plt from matplotlib.ticker import AutoMinorLocator %matplotlib inline  #matplotlib settings for Latex plots import matplotlib <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> <p>Next we load the pertinent classes from SAMBA.</p> In\u00a0[4]: Copied! <pre>#load SAMBA classes\nimport sys\nsys.path.append('../../')\n\nfrom samba.models import Models, Uncertainties\nfrom samba.discrepancy import Bivariate\n</pre> #load SAMBA classes import sys sys.path.append('../../')  from samba.models import Models, Uncertainties from samba.discrepancy import Bivariate <p>Let's choose $N_{s} = N_{l} = 2$ again for the first calculation, to compare to the results of the LMM method if desired.</p> In\u00a0[11]: Copied! <pre>#set up linspace and expansions\ng = np.linspace(1e-6,1.0,200)\nns = 2\nnl = 2\n</pre> #set up linspace and expansions g = np.linspace(1e-6,1.0,200) ns = 2 nl = 2 <p>Now we can instantiate an object for the Bivariate method.</p> In\u00a0[12]: Copied! <pre>#instantiate first object for N_s = N_l = 2\nmodel1 = Bivariate(ns, nl, error_model='informative')\n</pre> #instantiate first object for N_s = N_l = 2 model1 = Bivariate(ns, nl, error_model='informative') <p>Let's plot our functions again before we mix them.</p> In\u00a0[13]: Copied! <pre>#call plot_models() from Models() class\nplot_model = model1.plot_models(g)\n</pre> #call plot_models() from Models() class plot_model = model1.plot_models(g) <p>Now we want to mix the two expansions. All we will need is to call the plot_mix() function from the Bivariate() class, as it already calculates the necessary variances internally via the Uncertainties() class, and uses the fdagger() function internally as well.</p> <p>Expected values for mean1 and intervals1 below for $N_{s}=N_{l}=2$:</p> <p>mean1 = [2.50662827 2.50643831 2.50586856 2.50491903 2.50358972 ... 1.57507465 1.57205367 1.56905117 1.56606692 1.56310071]</p> <p>intervals1 = [[2.50662827 2.50662827] [2.50643825 2.50643836] [2.50586769 2.50586943]... [1.55288561 1.58521673] [1.55018531 1.58194852] [1.54749668 1.57870474]]</p> In\u00a0[14]: Copied! <pre>#call plot_mix() function to mix\nmean1, intervals1 = model1.plot_mix(g, plot_fdagger=True)\n</pre> #call plot_mix() function to mix mean1, intervals1 = model1.plot_mix(g, plot_fdagger=True) In\u00a0[15]: Copied! <pre>#check the mean and variance for N_s = N_l = 2 with the original values\nif ns == nl == 2:\n    mean_check = np.array([2.50662827, 2.50643831, 2.50586856, 2.50491903, 2.50358972,\n        2.50188062, 2.49979175, 2.49732309, 2.49447465, 2.49124643,\n        2.48763843, 2.48365064, 2.47928308, 2.47453573, 2.4694086 ,\n        2.4639017 , 2.45801502, 2.45174857, 2.44510236, 2.43807642,\n        2.43067076, 2.42288546, 2.41472057, 2.40617623, 2.39725262,\n        2.38795001, 2.3782688 , 2.36820957, 2.35777312, 2.3469606 ,\n        2.33577358, 2.32421423, 2.3122855 , 2.29999134, 2.28733708,\n        2.27432978, 2.26097875, 2.24729626, 2.23329829, 2.21900568,\n        2.20444538, 2.18965213, 2.17467054, 2.15955764, 2.14438598,\n        2.12924736, 2.11425728, 2.0995599 , 2.08533359, 2.07179644,\n        2.0592113 , 2.04788893, 2.03818779, 2.03050792, 2.02527619,\n        2.02291984, 2.02382653, 2.02829115, 2.03645436, 2.04824287,\n        2.06332654, 2.08110857, 2.10075991, 2.12129795, 2.1416953 ,\n        2.16099444, 2.17840257, 2.19334916, 2.20550125, 2.21474359,\n        2.22113648, 2.22486541, 2.22619265, 2.22541717, 2.22284479,\n        2.21876832, 2.21345579, 2.20714485, 2.20004125, 2.19232   ,\n        2.1841278 , 2.17558633, 2.16679557, 2.15783699, 2.14877658,\n        2.13966736, 2.13055169, 2.12146316, 2.11242816, 2.10346724,\n        2.09459618, 2.08582692, 2.07716827, 2.06862652, 2.06020597,\n        2.05190928, 2.04373783, 2.03569196, 2.02777122, 2.01997453,\n        2.0123003 , 2.00474659, 1.99731117, 1.98999159, 1.98278528,\n        1.97568957, 1.96870172, 1.96181898, 1.95503859, 1.94835782,\n        1.94177397, 1.93528439, 1.92888648, 1.9225777 , 1.9163556 ,\n        1.91021777, 1.90416189, 1.89818571, 1.89228705, 1.88646381,\n        1.88071395, 1.87503552, 1.86942662, 1.86388544, 1.8584102 ,\n        1.85299922, 1.84765086, 1.84236355, 1.83713578, 1.83196608,\n        1.82685304, 1.82179531, 1.81679158, 1.81184059, 1.80694111,\n        1.80209199, 1.79729208, 1.79254029, 1.78783557, 1.78317691,\n        1.77856331, 1.77399384, 1.76946756, 1.7649836 , 1.7605411 ,\n        1.75613922, 1.75177717, 1.74745417, 1.74316946, 1.73892232,\n        1.73471204, 1.73053794, 1.72639935, 1.72229564, 1.71822617,\n        1.71419035, 1.71018758, 1.70621731, 1.70227896, 1.69837201,\n        1.69449593, 1.69065022, 1.68683438, 1.68304792, 1.67929039,\n        1.67556132, 1.67186027, 1.66818682, 1.66454054, 1.66092102,\n        1.65732786, 1.65376068, 1.65021909, 1.64670272, 1.64321121,\n        1.63974422, 1.63630139, 1.63288239, 1.6294869 , 1.62611458,\n        1.62276514, 1.61943825, 1.61613363, 1.61285098, 1.60959002,\n        1.60635046, 1.60313203, 1.59993446, 1.5967575 , 1.59360088,\n        1.59046435, 1.58734767, 1.58425059, 1.58117288, 1.57811431,\n        1.57507465, 1.57205367, 1.56905117, 1.56606692, 1.56310071])\n    var_check = np.array([[2.50662827, 2.50662827],\n        [2.50643825, 2.50643836],\n        [2.50586769, 2.50586943],\n        [2.50491463, 2.50492343],\n        [2.50357583, 2.50360361],\n        [2.50184671, 2.50191454],\n        [2.49972143, 2.49986207],\n        [2.49719282, 2.49745336],\n        [2.49425242, 2.49469688],\n        [2.49089046, 2.4916024 ],\n        [2.48709588, 2.48818097],\n        [2.48285631, 2.48444498],\n        [2.47815807, 2.48040809],\n        [2.47298619, 2.47608527],\n        [2.46732441, 2.4714928 ],\n        [2.46115513, 2.46664827],\n        [2.45445949, 2.46157054],\n        [2.44721732, 2.45627982],\n        [2.43940713, 2.45079759],\n        [2.43100617, 2.44514667],\n        [2.42199037, 2.43935116],\n        [2.4123344 , 2.43343651],\n        [2.40201165, 2.42742949],\n        [2.39099425, 2.42135821],\n        [2.37925308, 2.41525216],\n        [2.36675782, 2.4091422 ],\n        [2.35347698, 2.40306063],\n        [2.33937793, 2.39704121],\n        [2.32442699, 2.39111926],\n        [2.30858951, 2.3853317 ],\n        [2.29182998, 2.37971718],\n        [2.27411223, 2.37431624],\n        [2.25539957, 2.36917142],\n        [2.23565511, 2.36432758],\n        [2.21484208, 2.35983208],\n        [2.19292434, 2.35573521],\n        [2.16986692, 2.35209059],\n        [2.14563685, 2.34895566],\n        [2.12020419, 2.3463924 ],\n        [2.09354332, 2.34446805],\n        [2.06563473, 2.34325603],\n        [2.03646723, 2.34283702],\n        [2.00604089, 2.34330019],\n        [1.97437074, 2.34474454],\n        [1.94149154, 2.34728041],\n        [1.90746376, 2.35103096],\n        [1.87238106, 2.35613349],\n        [1.83637937, 2.36274043],\n        [1.79964771, 2.37101947],\n        [1.76244058, 2.3811523 ],\n        [1.72509147, 2.39333113],\n        [1.68802624, 2.40775163],\n        [1.65177453, 2.42460104],\n        [1.61697608, 2.44403976],\n        [1.58437755, 2.46617483],\n        [1.55481471, 2.49102497],\n        [1.52917477, 2.51847829],\n        [1.50833527, 2.54824703],\n        [1.49308084, 2.57982789],\n        [1.48400579, 2.61247995],\n        [1.48141897, 2.64523411],\n        [1.48527312, 2.67694401],\n        [1.49514019, 2.70637962],\n        [1.51024509, 2.73235081],\n        [1.52955395, 2.75383666],\n        [1.55189735, 2.77009153],\n        [1.57609977, 2.78070538],\n        [1.60108853, 2.78560979],\n        [1.62596591, 2.7850366 ],\n        [1.65004101, 2.77944617],\n        [1.6728279 , 2.76944507],\n        [1.69402175, 2.75570906],\n        [1.71346423, 2.73892106],\n        [1.73110693, 2.7197274 ],\n        [1.74697836, 2.69871123],\n        [1.7611568 , 2.67637985],\n        [1.77374959, 2.65316199],\n        [1.78487826, 2.62941143],\n        [1.79466834, 2.60541417],\n        [1.80324285, 2.58139714],\n        [1.81071841, 2.55753719],\n        [1.81720314, 2.53396953],\n        [1.82279579, 2.51079534],\n        [1.82758564, 2.48808834],\n        [1.83165285, 2.46590031],\n        [1.83506906, 2.44426566],\n        [1.83789814, 2.42320524],\n        [1.84019698, 2.40272934],\n        [1.84201618, 2.38284014],\n        [1.8434008 , 2.36353368],\n        [1.84439101, 2.34480135],\n        [1.84502266, 2.32663118],\n        [1.84532777, 2.30900876],\n        [1.84533507, 2.29191798],\n        [1.8450703 , 2.27534165],\n        [1.84455664, 2.25926193],\n        [1.84381498, 2.24366067],\n        [1.84286419, 2.22851972],\n        [1.84172137, 2.21382107],\n        [1.84040202, 2.19954703],\n        [1.83892024, 2.18568036],\n        [1.83728889, 2.17220429],\n        [1.8355197 , 2.15910263],\n        [1.83362341, 2.14635977],\n        [1.83160985, 2.13396072],\n        [1.82948805, 2.12189108],\n        [1.82726634, 2.1101371 ],\n        [1.82495235, 2.0986856 ],\n        [1.82255315, 2.08752403],\n        [1.82007526, 2.07664038],\n        [1.81752472, 2.06602322],\n        [1.81490711, 2.05566167],\n        [1.81222762, 2.04554534],\n        [1.80949105, 2.03566435],\n        [1.80670189, 2.02600931],\n        [1.80386429, 2.01657125],\n        [1.80098212, 2.00734165],\n        [1.79805901, 1.99831241],\n        [1.79509831, 1.98947579],\n        [1.79210318, 1.98082444],\n        [1.78907655, 1.97235136],\n        [1.78602117, 1.96404988],\n        [1.7829396 , 1.95591364],\n        [1.77983426, 1.94793661],\n        [1.7767074 , 1.94011299],\n        [1.77356113, 1.93243731],\n        [1.77039742, 1.9249043 ],\n        [1.76721814, 1.91750897],\n        [1.76402501, 1.91024655],\n        [1.76081969, 1.90311247],\n        [1.7576037 , 1.89610239],\n        [1.75437848, 1.88921215],\n        [1.75114538, 1.88243778],\n        [1.74790569, 1.87577548],\n        [1.74466059, 1.86922164],\n        [1.7414112 , 1.86277277],\n        [1.73815859, 1.85642556],\n        [1.73490375, 1.85017683],\n        [1.73164761, 1.84402354],\n        [1.72839105, 1.83796277],\n        [1.72513489, 1.83199174],\n        [1.72187992, 1.82610776],\n        [1.71862686, 1.82030827],\n        [1.71537639, 1.81459081],\n        [1.71212918, 1.80895302],\n        [1.70888581, 1.80339263],\n        [1.70564687, 1.79790747],\n        [1.70241289, 1.79249544],\n        [1.69918438, 1.78715454],\n        [1.6959618 , 1.78188284],\n        [1.6927456 , 1.77667848],\n        [1.68953619, 1.77153968],\n        [1.68633398, 1.76646471],\n        [1.68313933, 1.76145194],\n        [1.67995258, 1.75649976],\n        [1.67677405, 1.75160665],\n        [1.67360404, 1.74677113],\n        [1.67044285, 1.74199177],\n        [1.66729072, 1.7372672 ],\n        [1.66414791, 1.73259611],\n        [1.66101465, 1.72797721],\n        [1.65789116, 1.72340928],\n        [1.65477763, 1.71889113],\n        [1.65167424, 1.7144216 ],\n        [1.64858118, 1.70999959],\n        [1.6454986 , 1.70562404],\n        [1.64242664, 1.7012939 ],\n        [1.63936546, 1.69700818],\n        [1.63631517, 1.69276591],\n        [1.63327588, 1.68856616],\n        [1.63024772, 1.68440801],\n        [1.62723076, 1.68029059],\n        [1.62422511, 1.67621306],\n        [1.62123085, 1.67217459],\n        [1.61824803, 1.66817439],\n        [1.61527675, 1.66421169],\n        [1.61231704, 1.66028574],\n        [1.60936897, 1.65639582],\n        [1.60643258, 1.65254122],\n        [1.60350791, 1.64872126],\n        [1.60059499, 1.64493528],\n        [1.59769386, 1.64118265],\n        [1.59480453, 1.63746273],\n        [1.59192704, 1.63377493],\n        [1.58906139, 1.63011865],\n        [1.58620759, 1.62649333],\n        [1.58336565, 1.62289841],\n        [1.58053558, 1.61933335],\n        [1.57771737, 1.61579763],\n        [1.57491101, 1.61229074],\n        [1.57211651, 1.60881218],\n        [1.56933385, 1.60536148],\n        [1.56656302, 1.60193816],\n        [1.563804  , 1.59854176],\n        [1.56105677, 1.59517185],\n        [1.55832131, 1.59182799],\n        [1.5555976 , 1.58850975],\n        [1.55288561, 1.58521673],\n        [1.55018531, 1.58194852],\n        [1.54749668, 1.57870474]])\n    \n    mean_diff = np.abs(mean1 - mean_check)\n    var_diff = np.abs(intervals1 - var_check)\n    \n    #set tolerance\n    abs_tol = 1e-8\n\n    #check all elements of the arrays\n    for i in range(len(mean_diff)):\n\n        #check mean\n        if mean_diff[i] &lt; abs_tol:\n            mean_ans = 'True'\n        else:\n            mean_ans = 'False'\n\n        #check intervals\n        if var_diff[i,0] &lt; abs_tol:\n            var_ans_1 = 'True'\n        else: \n            var_ans_1 = 'False'\n        if var_diff[i, 1] &lt; abs_tol:\n            var_ans_2 = 'True'\n        else: \n            var_ans_2 = 'False'\n        if var_ans_1 == 'True' and var_ans_2 == 'True':\n            var_ans = 'True'\n        elif var_ans_1 != 'True' or var_ans_2 != 'True':\n            var_ans = 'False'\n        \n    #print test results\n    print('Mean passing?', mean_ans)\n    print('Intervals passing?', var_ans)\n    \nelse:\n    pass\n</pre> #check the mean and variance for N_s = N_l = 2 with the original values if ns == nl == 2:     mean_check = np.array([2.50662827, 2.50643831, 2.50586856, 2.50491903, 2.50358972,         2.50188062, 2.49979175, 2.49732309, 2.49447465, 2.49124643,         2.48763843, 2.48365064, 2.47928308, 2.47453573, 2.4694086 ,         2.4639017 , 2.45801502, 2.45174857, 2.44510236, 2.43807642,         2.43067076, 2.42288546, 2.41472057, 2.40617623, 2.39725262,         2.38795001, 2.3782688 , 2.36820957, 2.35777312, 2.3469606 ,         2.33577358, 2.32421423, 2.3122855 , 2.29999134, 2.28733708,         2.27432978, 2.26097875, 2.24729626, 2.23329829, 2.21900568,         2.20444538, 2.18965213, 2.17467054, 2.15955764, 2.14438598,         2.12924736, 2.11425728, 2.0995599 , 2.08533359, 2.07179644,         2.0592113 , 2.04788893, 2.03818779, 2.03050792, 2.02527619,         2.02291984, 2.02382653, 2.02829115, 2.03645436, 2.04824287,         2.06332654, 2.08110857, 2.10075991, 2.12129795, 2.1416953 ,         2.16099444, 2.17840257, 2.19334916, 2.20550125, 2.21474359,         2.22113648, 2.22486541, 2.22619265, 2.22541717, 2.22284479,         2.21876832, 2.21345579, 2.20714485, 2.20004125, 2.19232   ,         2.1841278 , 2.17558633, 2.16679557, 2.15783699, 2.14877658,         2.13966736, 2.13055169, 2.12146316, 2.11242816, 2.10346724,         2.09459618, 2.08582692, 2.07716827, 2.06862652, 2.06020597,         2.05190928, 2.04373783, 2.03569196, 2.02777122, 2.01997453,         2.0123003 , 2.00474659, 1.99731117, 1.98999159, 1.98278528,         1.97568957, 1.96870172, 1.96181898, 1.95503859, 1.94835782,         1.94177397, 1.93528439, 1.92888648, 1.9225777 , 1.9163556 ,         1.91021777, 1.90416189, 1.89818571, 1.89228705, 1.88646381,         1.88071395, 1.87503552, 1.86942662, 1.86388544, 1.8584102 ,         1.85299922, 1.84765086, 1.84236355, 1.83713578, 1.83196608,         1.82685304, 1.82179531, 1.81679158, 1.81184059, 1.80694111,         1.80209199, 1.79729208, 1.79254029, 1.78783557, 1.78317691,         1.77856331, 1.77399384, 1.76946756, 1.7649836 , 1.7605411 ,         1.75613922, 1.75177717, 1.74745417, 1.74316946, 1.73892232,         1.73471204, 1.73053794, 1.72639935, 1.72229564, 1.71822617,         1.71419035, 1.71018758, 1.70621731, 1.70227896, 1.69837201,         1.69449593, 1.69065022, 1.68683438, 1.68304792, 1.67929039,         1.67556132, 1.67186027, 1.66818682, 1.66454054, 1.66092102,         1.65732786, 1.65376068, 1.65021909, 1.64670272, 1.64321121,         1.63974422, 1.63630139, 1.63288239, 1.6294869 , 1.62611458,         1.62276514, 1.61943825, 1.61613363, 1.61285098, 1.60959002,         1.60635046, 1.60313203, 1.59993446, 1.5967575 , 1.59360088,         1.59046435, 1.58734767, 1.58425059, 1.58117288, 1.57811431,         1.57507465, 1.57205367, 1.56905117, 1.56606692, 1.56310071])     var_check = np.array([[2.50662827, 2.50662827],         [2.50643825, 2.50643836],         [2.50586769, 2.50586943],         [2.50491463, 2.50492343],         [2.50357583, 2.50360361],         [2.50184671, 2.50191454],         [2.49972143, 2.49986207],         [2.49719282, 2.49745336],         [2.49425242, 2.49469688],         [2.49089046, 2.4916024 ],         [2.48709588, 2.48818097],         [2.48285631, 2.48444498],         [2.47815807, 2.48040809],         [2.47298619, 2.47608527],         [2.46732441, 2.4714928 ],         [2.46115513, 2.46664827],         [2.45445949, 2.46157054],         [2.44721732, 2.45627982],         [2.43940713, 2.45079759],         [2.43100617, 2.44514667],         [2.42199037, 2.43935116],         [2.4123344 , 2.43343651],         [2.40201165, 2.42742949],         [2.39099425, 2.42135821],         [2.37925308, 2.41525216],         [2.36675782, 2.4091422 ],         [2.35347698, 2.40306063],         [2.33937793, 2.39704121],         [2.32442699, 2.39111926],         [2.30858951, 2.3853317 ],         [2.29182998, 2.37971718],         [2.27411223, 2.37431624],         [2.25539957, 2.36917142],         [2.23565511, 2.36432758],         [2.21484208, 2.35983208],         [2.19292434, 2.35573521],         [2.16986692, 2.35209059],         [2.14563685, 2.34895566],         [2.12020419, 2.3463924 ],         [2.09354332, 2.34446805],         [2.06563473, 2.34325603],         [2.03646723, 2.34283702],         [2.00604089, 2.34330019],         [1.97437074, 2.34474454],         [1.94149154, 2.34728041],         [1.90746376, 2.35103096],         [1.87238106, 2.35613349],         [1.83637937, 2.36274043],         [1.79964771, 2.37101947],         [1.76244058, 2.3811523 ],         [1.72509147, 2.39333113],         [1.68802624, 2.40775163],         [1.65177453, 2.42460104],         [1.61697608, 2.44403976],         [1.58437755, 2.46617483],         [1.55481471, 2.49102497],         [1.52917477, 2.51847829],         [1.50833527, 2.54824703],         [1.49308084, 2.57982789],         [1.48400579, 2.61247995],         [1.48141897, 2.64523411],         [1.48527312, 2.67694401],         [1.49514019, 2.70637962],         [1.51024509, 2.73235081],         [1.52955395, 2.75383666],         [1.55189735, 2.77009153],         [1.57609977, 2.78070538],         [1.60108853, 2.78560979],         [1.62596591, 2.7850366 ],         [1.65004101, 2.77944617],         [1.6728279 , 2.76944507],         [1.69402175, 2.75570906],         [1.71346423, 2.73892106],         [1.73110693, 2.7197274 ],         [1.74697836, 2.69871123],         [1.7611568 , 2.67637985],         [1.77374959, 2.65316199],         [1.78487826, 2.62941143],         [1.79466834, 2.60541417],         [1.80324285, 2.58139714],         [1.81071841, 2.55753719],         [1.81720314, 2.53396953],         [1.82279579, 2.51079534],         [1.82758564, 2.48808834],         [1.83165285, 2.46590031],         [1.83506906, 2.44426566],         [1.83789814, 2.42320524],         [1.84019698, 2.40272934],         [1.84201618, 2.38284014],         [1.8434008 , 2.36353368],         [1.84439101, 2.34480135],         [1.84502266, 2.32663118],         [1.84532777, 2.30900876],         [1.84533507, 2.29191798],         [1.8450703 , 2.27534165],         [1.84455664, 2.25926193],         [1.84381498, 2.24366067],         [1.84286419, 2.22851972],         [1.84172137, 2.21382107],         [1.84040202, 2.19954703],         [1.83892024, 2.18568036],         [1.83728889, 2.17220429],         [1.8355197 , 2.15910263],         [1.83362341, 2.14635977],         [1.83160985, 2.13396072],         [1.82948805, 2.12189108],         [1.82726634, 2.1101371 ],         [1.82495235, 2.0986856 ],         [1.82255315, 2.08752403],         [1.82007526, 2.07664038],         [1.81752472, 2.06602322],         [1.81490711, 2.05566167],         [1.81222762, 2.04554534],         [1.80949105, 2.03566435],         [1.80670189, 2.02600931],         [1.80386429, 2.01657125],         [1.80098212, 2.00734165],         [1.79805901, 1.99831241],         [1.79509831, 1.98947579],         [1.79210318, 1.98082444],         [1.78907655, 1.97235136],         [1.78602117, 1.96404988],         [1.7829396 , 1.95591364],         [1.77983426, 1.94793661],         [1.7767074 , 1.94011299],         [1.77356113, 1.93243731],         [1.77039742, 1.9249043 ],         [1.76721814, 1.91750897],         [1.76402501, 1.91024655],         [1.76081969, 1.90311247],         [1.7576037 , 1.89610239],         [1.75437848, 1.88921215],         [1.75114538, 1.88243778],         [1.74790569, 1.87577548],         [1.74466059, 1.86922164],         [1.7414112 , 1.86277277],         [1.73815859, 1.85642556],         [1.73490375, 1.85017683],         [1.73164761, 1.84402354],         [1.72839105, 1.83796277],         [1.72513489, 1.83199174],         [1.72187992, 1.82610776],         [1.71862686, 1.82030827],         [1.71537639, 1.81459081],         [1.71212918, 1.80895302],         [1.70888581, 1.80339263],         [1.70564687, 1.79790747],         [1.70241289, 1.79249544],         [1.69918438, 1.78715454],         [1.6959618 , 1.78188284],         [1.6927456 , 1.77667848],         [1.68953619, 1.77153968],         [1.68633398, 1.76646471],         [1.68313933, 1.76145194],         [1.67995258, 1.75649976],         [1.67677405, 1.75160665],         [1.67360404, 1.74677113],         [1.67044285, 1.74199177],         [1.66729072, 1.7372672 ],         [1.66414791, 1.73259611],         [1.66101465, 1.72797721],         [1.65789116, 1.72340928],         [1.65477763, 1.71889113],         [1.65167424, 1.7144216 ],         [1.64858118, 1.70999959],         [1.6454986 , 1.70562404],         [1.64242664, 1.7012939 ],         [1.63936546, 1.69700818],         [1.63631517, 1.69276591],         [1.63327588, 1.68856616],         [1.63024772, 1.68440801],         [1.62723076, 1.68029059],         [1.62422511, 1.67621306],         [1.62123085, 1.67217459],         [1.61824803, 1.66817439],         [1.61527675, 1.66421169],         [1.61231704, 1.66028574],         [1.60936897, 1.65639582],         [1.60643258, 1.65254122],         [1.60350791, 1.64872126],         [1.60059499, 1.64493528],         [1.59769386, 1.64118265],         [1.59480453, 1.63746273],         [1.59192704, 1.63377493],         [1.58906139, 1.63011865],         [1.58620759, 1.62649333],         [1.58336565, 1.62289841],         [1.58053558, 1.61933335],         [1.57771737, 1.61579763],         [1.57491101, 1.61229074],         [1.57211651, 1.60881218],         [1.56933385, 1.60536148],         [1.56656302, 1.60193816],         [1.563804  , 1.59854176],         [1.56105677, 1.59517185],         [1.55832131, 1.59182799],         [1.5555976 , 1.58850975],         [1.55288561, 1.58521673],         [1.55018531, 1.58194852],         [1.54749668, 1.57870474]])          mean_diff = np.abs(mean1 - mean_check)     var_diff = np.abs(intervals1 - var_check)          #set tolerance     abs_tol = 1e-8      #check all elements of the arrays     for i in range(len(mean_diff)):          #check mean         if mean_diff[i] &lt; abs_tol:             mean_ans = 'True'         else:             mean_ans = 'False'          #check intervals         if var_diff[i,0] &lt; abs_tol:             var_ans_1 = 'True'         else:              var_ans_1 = 'False'         if var_diff[i, 1] &lt; abs_tol:             var_ans_2 = 'True'         else:              var_ans_2 = 'False'         if var_ans_1 == 'True' and var_ans_2 == 'True':             var_ans = 'True'         elif var_ans_1 != 'True' or var_ans_2 != 'True':             var_ans = 'False'              #print test results     print('Mean passing?', mean_ans)     print('Intervals passing?', var_ans)      else:     pass <pre>Mean passing? True\nIntervals passing? True\n</pre> <p>And, as simple as that, there is the mixed model! The green curve is the PPD from the $f^{\\dagger}$ equation at the top of this notebook. The credibility interval, in shaded green, is quite large, as was discussed at length in the paper in Section IV.</p> <p>In order to fix this (in our toy case) overly conservative band is to implement a GP in the center to take care of some of the interpolation there, where we do not have any prior information other than the variances of the series expansions (shown in dotted red and blue above). That will be the subject of the next notebook, but first let's also look at another case: $N_{s} = N_{l} = 5$.</p> <p>We begin by instantiating another object for this new choice of models. We again select 'i' for the informative error model when the question is asked.</p> In\u00a0[16]: Copied! <pre>#instantiate a new object for N_s = N_l = 5\nns = 5\nnl = 5\nmodel2 = Bivariate(ns, nl, error_model='informative')\n</pre> #instantiate a new object for N_s = N_l = 5 ns = 5 nl = 5 model2 = Bivariate(ns, nl, error_model='informative') <p>Now we can simply call plot_mix() again to plot the results of mixing these two expansions.</p> <p>Expected values for mean2 and intervals2 below for $N_{s}=N_{l}=5$:</p> <p>mean2 = [2.50662827 2.50643839 2.5058699  2.50492583 2.5036112...1.56617848 1.56330958 1.56045582 1.55761707 1.55479321]</p> <p>intervals2 = [[2.50662827 2.50662827] [2.50643839 2.50643839] [2.5058699  2.50586991] ... [1.56040879 1.56050285] [1.55757156 1.55766258] [1.55474917 1.55483725]]</p> In\u00a0[17]: Copied! <pre>#call plot_mix()\nmean2, intervals2 = model2.plot_mix(g, plot_fdagger=True)\n</pre> #call plot_mix() mean2, intervals2 = model2.plot_mix(g, plot_fdagger=True) <p>This mixed model result is quite similar to the one for $N_{s} = N_{l} = 2$, but with a smaller gap, so a smaller uncertainty band overall. Let's look next at a comparison between the two theory error models developed in the paper.</p> <p>In the paper, we discuss (in Section IIB) that there are multiple different ways of quantifying uncertainty on these series expansions, two of which we have coded and at our disposal now. The one we used above is dependent on which letter you typed for the error model to be chosen: u (uninformative) or i (informative). These error models have slightly different shapes when it comes to their uncertainty bands, as will be demonstrated below.</p> <p>First we instantiate another object from the Bivariate method for $N_{s} = N_{l} = 3$, as this case makes the differences quite clear between the uninformative and informative error models. We type 'u' or 'i' for this first instance, as it does not matter for our comparison plot, and is only setting the error model for the rest of the object (which we will not be using anyway).</p> In\u00a0[18]: Copied! <pre>#instantiate a third object for N_s = N_l = 3\nns = 3\nnl = 3\nmodel3 = Bivariate(ns, nl, error_model='informative')\n</pre> #instantiate a third object for N_s = N_l = 3 ns = 3 nl = 3 model3 = Bivariate(ns, nl, error_model='informative') <p>We begin (and end, as everything is done in one function) by calling the plot_error_models() function from the Bivariate() class, and type 'u' for the first panel, and 'i' for the second panel.</p> In\u00a0[19]: Copied! <pre>#plot the uninformative and informative error models\nmodel3.plot_error_models(g)\n</pre> #plot the uninformative and informative error models model3.plot_error_models(g) <p>It is not too obvious upon first glance, but it can be seen that panel (a) possesses a larger uncertainty band than panel (b) does. This is the subtle difference in the error models coming into play---panel (b) is using an error model that knows more about the next term in the series than panel (a)'s model does, so the uncertainty in the gap in panel (b) is a little smaller. However, they're both still pretty large for our toy case. Now we should try the GP and see what kind of help it provides---see the next notebook (GP_BMM) for details on how to include a Gaussian Process in this mixed model.</p> <p>Written by: Alexandra Semposki (06 June 2022)</p>"},{"location":"Tutorials/Bivariate_BMM/#an-introduction-to-samba-bivariate-bayesian-model-mixing","title":"An introduction to SAMBA: Bivariate Bayesian Model Mixing\u00b6","text":"<p>Alexandra Semposki</p> <p>Date: 06 June 2022</p>"},{"location":"Tutorials/Bivariate_BMM/#introduction","title":"Introduction\u00b6","text":""},{"location":"Tutorials/Bivariate_BMM/#bivariate-model-mixing-ppd","title":"Bivariate model mixing: PPD\u00b6","text":""},{"location":"Tutorials/Bivariate_BMM/#error-model-comparison","title":"Error Model Comparison\u00b6","text":""},{"location":"Tutorials/GP_BMM/","title":"An introduction to SAMBA: Trivariate Bayesian Model Mixing w/GPs","text":"<p>From the last notebook, you saw the bivariate model mixing produce a well-mixed model with an overly conservative uncertainty band in the gap (for our toy model). We now want to try mixing in an interpolant in the form of a Gaussian Process (GP). This will enter $f^{\\dagger}$ as a third individual model. Recall the function for $f^\\dagger$:</p> <p>$$ f^{\\dagger} \\sim \\bigl(Z_P^{-1}\\sum_k \\frac{1}{v_k}f_k, Z_P^{-1}\\bigr), $$</p> <p>where</p> <p>$$ Z_P \\equiv \\sum_{k=1}^{K}\\frac{1}{v_k}. $$</p> <p>$Z_P$ is the precision, or the inverse of the variance, of the $K$ models, $v_{k}$ the individual variances of each model (which we previously denoted the theory error), and $f^{\\dagger}$ the mixed model. Now $K = 3$ for this trivariate case.</p> <p>We start by loading all of our necessary packages and SAMBA classes.</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\n#import packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom matplotlib.ticker import AutoMinorLocator\n%matplotlib inline\n\n#matplotlib settings for Latex plots\nimport matplotlib\n</pre> %load_ext autoreload %autoreload 2  #import packages import numpy as np import matplotlib.pyplot as plt from scipy import stats from matplotlib.ticker import AutoMinorLocator %matplotlib inline  #matplotlib settings for Latex plots import matplotlib In\u00a0[2]: Copied! <pre>#import the SAMBA classes needed\nimport sys\nsys.path.append('../../')\n\nfrom samba.models import Models, Uncertainties\nfrom samba.discrepancy import Bivariate\nfrom samba.gaussprocess import GP\nfrom samba.fprdat import FPR\n</pre> #import the SAMBA classes needed import sys sys.path.append('../../')  from samba.models import Models, Uncertainties from samba.discrepancy import Bivariate from samba.gaussprocess import GP from samba.fprdat import FPR <p>As usual, we start by defining two series expansions to mix. We will begin with $N_{s} = N_{l} = 3$. (Note that we could mix more than two series expansions, but we're sticking with the bare minimum in this tutorial.)</p> In\u00a0[16]: Copied! <pre>#define g and series expansions\ng = np.linspace(1e-6,1.0,100)\nns = 3\nnl = 3\n</pre> #define g and series expansions g = np.linspace(1e-6,1.0,100) ns = 3 nl = 3 In\u00a0[17]: Copied! <pre># run something simple for array to check and save in a txt file\nobj = Models(ns,nl)\nobjvar = Uncertainties(error_model='informative')\nlow_val = obj.low_g(g)[0]\nhigh_val = obj.high_g(g)[0]\nlow_std = objvar.variance_low(g, ns)\nhigh_std = objvar.variance_high(g, nl)\n\nmodel1 = Bivariate(ns, nl, error_model='informative')\nmean1, intervals1 = model1.plot_mix(g, plot_fdagger=True)\nstd_dev = intervals1[:,1] - mean1\n\n# obtain the weights and then store them\nweights = model1.var_weights\nweights_low = weights[0,:]\nweights_high = weights[1,:]\n\n# concatenate them / list format arrays\nsamba_values = np.array([low_val, high_val, low_std, high_std, mean1, intervals1[:,0], intervals1[:,1], std_dev, weights_low, weights_high])\nsamba_arrays = np.savetxt('samba_results.txt', samba_values, delimiter=',')\n</pre> # run something simple for array to check and save in a txt file obj = Models(ns,nl) objvar = Uncertainties(error_model='informative') low_val = obj.low_g(g)[0] high_val = obj.high_g(g)[0] low_std = objvar.variance_low(g, ns) high_std = objvar.variance_high(g, nl)  model1 = Bivariate(ns, nl, error_model='informative') mean1, intervals1 = model1.plot_mix(g, plot_fdagger=True) std_dev = intervals1[:,1] - mean1  # obtain the weights and then store them weights = model1.var_weights weights_low = weights[0,:] weights_high = weights[1,:]  # concatenate them / list format arrays samba_values = np.array([low_val, high_val, low_std, high_std, mean1, intervals1[:,0], intervals1[:,1], std_dev, weights_low, weights_high]) samba_arrays = np.savetxt('samba_results.txt', samba_values, delimiter=',') <p>Now we instantiate the GP class, which acts as a wrapper for the Python package scikit learn. We immediately pick the kernel we wish to use for the GP here. The options are RBF, Matern, and Rational Quadratic. If we want to pick Matern, we also supply the value of $\\nu$ when asked. This is shown below.</p> In\u00a0[18]: Copied! <pre>#instantiate GP object for N_s = N_l = 3\nobj1 = GP(g, ns, nl, kernel=\"Matern\", nu=1.5, ci=68, error_model='informative')\n</pre> #instantiate GP object for N_s = N_l = 3 obj1 = GP(g, ns, nl, kernel=\"Matern\", nu=1.5, ci=68, error_model='informative') <p>Now we have a GP object that will use the Matern 3/2 kernel for its analysis. When using a GP, we need to train on a set of data. In a typical situation where the user has data to give the GP, these would be the training points used. However, in our case, with a toy model, we must generate these training points. In SAMBA, the training points are chosen by offsetting the original linspace in $g$ by a small amount, so a completely new linspace in $g$ is created that does not overlap exactly with any of the points first given to the GP class. This is because those points will be used in both the series expansion calculation and the GP prediction---which means, if we want the series expansions and GP to mix correctly in $f^\\dagger$, we need to have the points in $g$ for all three models to be the same, or we will not be mixing the right points. Hence, we create a training set after a prediction set for the toy model, unlike in real situations.</p> <p>SAMBA takes care of generating this new training array for you, but it does request that you choose a method to determine the fourth and final training point, located within the region of the large-$g$ expansion. This is because there are many different ways we could assign that training point. The three methods that we tried are</p> <ul> <li><p>Fixing the point at $g = 0.6$ (independent of the location of the gap)</p> </li> <li><p>Choosing the point in the array computed for the large-$g$ expansion where the value of $F_s(g)$ cuts off (where we choose to cut off the calculation of the small-$g$ expansion at the edge of the pertinent domain in $F(g)$)</p> </li> <li><p>Putting the training point at the location where the theory error on the current point in the large-$g$ expansion array is larger than 5% of the next value of $F_l(g)$ in the array</p> </li> </ul> <p>SAMBA places the other 3 points at fixed points: two in the small-$g$ expansion array, and one at $g = 1$ in the large-$g$ expansion.</p> <p>In our tutorial, we pick method 2, which tends to yield the best results when comparing the true curve to the mixed model. However, none of the 3 methods are spectacularly better than the others, which shows how the GP is not overly dependent on the placement of the training points (great news for generalizing this!). We also select 'error=True' in the arguments of the training() function, because we wish to use the theory errors in our GP.</p> <p>Expected Gaussian parameters for method 2 with Matern 3/2 kernel and $N_{s}=N_{l}=3$ below:</p> <p>length_scale = 2.08</p> <p>variance = 2.27</p> In\u00a0[19]: Copied! <pre>#call the training() function from the GP() class\nobj_tr1 = obj1.training(error=True, method=2, plot=True)\n</pre> #call the training() function from the GP() class obj_tr1 = obj1.training(error=True, method=2, plot=True) <pre>Gaussian process parameters: 2.3**2 * Matern(length_scale=2.14, nu=1.5)\n</pre> <p>The plot above shows the theory errors on each calculated training point, with the red and blue curves being the small-$g$ and large-$g$ expansions, as usual. The black points are the four chosen training points for our GP that it just used to train itself. We will now use the validate() function in the GP class to predict at each original point in $g$.</p> <p>Expected mean1, sig1, cov1 for method 2, Matern 3/2 kernel and $N_{s}=N_{l}=3$ below:</p> <p>mean1 = [2.55678727 2.550882   2.54491822 2.53889552 2.53281353 ...1.56303294 1.56056141 1.55811837 1.55570382 1.55331773]</p> <p>sig1 = [0.0193262  0.01764195 0.01599236 0.01437896 0.01280341...0.00835179 0.00663118 0.00485759 0.00305163 0.00135394]</p> <p>cov1 = [[3.73502044e-04 3.40524975e-04 3.07502834e-04 ... 4.08375077e-07 2.44743834e-07 8.20266537e-08] [3.40524975e-04 3.11238518e-04 2.81740028e-04 ... 3.80396282e-07 2.27975826e-07 7.64068068e-08] ... [2.44743834e-07 2.27975826e-07 2.11062770e-07 ... 1.46638192e-05 9.31247488e-06 3.69959419e-06] [8.20266537e-08 7.64068068e-08 7.07383467e-08 ... 5.46592671e-06 3.69959419e-06 1.83315966e-06]]</p> In\u00a0[20]: Copied! <pre>#call the validate() function\nmean1, sig1, cov1 = obj1.validate(plot=True)\n</pre> #call the validate() function mean1, sig1, cov1 = obj1.validate(plot=True) <p>We can now see the shaded green region and green curve in the plot above, which correspond to the GP variance band and GP mean curve, respectively.</p> <p>Expected values of mixed_mean, mixed_intervals for Matern 3/2 kernel, method 2, with $N_{s}=N_{l}=3$ below:</p> <p>mixed_mean = [2.50662827 2.50643831 2.50586856 2.50491903 2.50358975 ... 1.56463932 1.56177765 1.55892173 1.55608208 1.55336891]</p> <p>mixed_intervals = [[2.50662827 2.50662827] [2.50643825 2.50643836] [2.50586769 2.50586943] ... [1.55670377 1.56113969] [1.55417787 1.55798629] [1.5521918  1.55454602]]</p> In\u00a0[21]: Copied! <pre>#call plot_mix() to mix in the GP\nmixed_mean, mixed_intervals = obj1.plot_mix(g, plot_fdagger=True, plot_true=True, GP_mean=mean1, GP_var=np.square(sig1))\n</pre> #call plot_mix() to mix in the GP mixed_mean, mixed_intervals = obj1.plot_mix(g, plot_fdagger=True, plot_true=True, GP_mean=mean1, GP_var=np.square(sig1)) <p>This result looks WAY better in the gap than the bivariate BMM did alone! Now let's directly compare to that case using our comparison function subplot_mix(), also found in the Bivariate() class.</p> In\u00a0[22]: Copied! <pre>#call subplot_mix() and plot no GP results next to GP results\nobj1.subplot_mix(g, GP_mean=mean1, GP_var=np.square(sig1))\n</pre> #call subplot_mix() and plot no GP results next to GP results obj1.subplot_mix(g, GP_mean=mean1, GP_var=np.square(sig1)) <p>The GP shows an obvious improvement in the gap in panel (b), as it follows the true curve quite well. The uncertainty band is also more believable, as it encompasses most of the true curve and does not allow for the possibility of any surprising result (like the dip seen in panel (a), which might be possible for a physical situation but not in a toy case where we know exactly what we are expecting to get).</p> <p>Let's check our results and see if we can truly trust them. One good way to do this is our diagnostic, the Mahalanobis distance. This is given by</p> <p>$$ \\mathrm{D}^{2}_{\\mathrm{MD}} = (y - m)^{T}\\textit{K}^{-1}(y - m), $$</p> <p>where $y$ is the vector of true solutions at each point in $g$ that we are comparing to (our true curve), $m$ is the GP solution at each point in $g$, and $K^{-1}$ is the inverse of the GP covariance matrix, returned by our validate() function above.</p> <p>Let's try this calculation to see what we get for the Mahalanobis distance. We will use 3 points to calculate it, as our lengthscale is quite long, and we can only put a few points in it without $K$ becoming a singular matrix. We will also choose 1000 draws from our GP to compare our Mahalanobis distance to a reference distribution.</p> <p>Expected values for md_g, md_mean, md_sig, and md_cov for pts=3, $N_{s}=N_{l}=3$, Matern 3/2 kernel, method 2 below:</p> <p>md_g = [0.29648312 0.52763866 0.76381933]</p> <p>md_mean = [2.16914526 1.90627322 1.69666482]</p> <p>md_sig = [0.04170691 0.04040816 0.02373173]</p> <p>md_cov = [[ 0.00173947  0.00111714 -0.00027924] [ 0.00111714  0.00163282 -0.00048348] [-0.00027924 -0.00048348  0.00056319]]</p> <p>Expected values for md_gp, md_ref below:</p> <p>md_gp = 0.2452667195670961</p> <p>md_ref = [ 2.18957992  2.51578661  0.71175949  1.7594288   2.12702599 ... 2.92643679 1.28506737  0.50875747  2.12148018  4.50742567]</p> In\u00a0[23]: Copied! <pre>#calculate the Mahalanobis points\nmd_g, md_mean, md_sig, md_cov = obj1.MD_set(pts=3, plot=True)\n\n#use the points to calculate the Mahalanobis distance for our GP\nmd_gp, md_ref = obj1.md_squared(md_g, md_mean, md_cov, n_curves=1000)\n</pre> #calculate the Mahalanobis points md_g, md_mean, md_sig, md_cov = obj1.MD_set(pts=3, plot=True)  #use the points to calculate the Mahalanobis distance for our GP md_gp, md_ref = obj1.md_squared(md_g, md_mean, md_cov, n_curves=1000) <p>Now that we have all of this information, let's plot the histogram of our GP curves to double check that we are indeed getting the $\\chi^{2}$ distribution shape we expect from our GP draws.</p> In\u00a0[24]: Copied! <pre>#call \nhelp(obj1.md_plotter)\nobj1.md_plotter(md_gp, md_ref, md_mean, md_cov, hist=True, box=False)\n</pre> #call  help(obj1.md_plotter) obj1.md_plotter(md_gp, md_ref, md_mean, md_cov, hist=True, box=False) <pre>Help on method md_plotter in module samba.gaussprocess:\n\nmd_plotter(md_gp, md_ref, md_mean=None, md_cov=None, hist=True, box=False) method of samba.gaussprocess.GP instance\n    A plotting function that allows the Mahalanobis distance\n    to be plotted using either a histogram or a box and whisker\n    plot, or both. \n    \n    Box and whisker plot code heavily drawn from J. Melendez' gsum\n    code (https://github.com/buqeye/gsum).\n    \n    :Example:\n        GP.md_plotter(md_gp=np.array([]), md_ref=np.array([]),\n        hist=False, box=True)\n    \n    Parameters:\n    -----------\n    md_gp : float\n        The MD^2 value for the GP curve. \n    \n    md_ref : numpy.ndarray\n        The array of MD^2 values for the reference\n        distribution.\n    \n    md_mean : numpy.ndarray\n        The values of the GP mean at the md_g points. Only used\n        for box and whisker option; default is None. \n    \n    md_cov : numpy.ndarray\n        The values of the GP covariance matrix at the md_g points. \n        Only used for box and whisker option; default is None.\n    \n    hist : bool\n        Toggle for plotting a histogram. Default is True. \n    \n    box : bool\n        Toggle for plotting a box plot. Default is False. \n    \n    Returns:\n    --------\n    None.\n\n</pre> <p>This looks good; the red dot at the low end is the squared MD result from our GP. It is close to 0, indicating that the result we get is not too far from the true curve, and it is within the expectation from the $\\chi^{2}$, so we have a good prediction for our mixed model.</p> In\u00a0[25]: Copied! <pre>#pull weights out of fdagger()\nvargp1 = obj1.var_weights[0]\nvargp2 = obj1.var_weights[1]\nvargp3 = obj1.var_weights[2]\n\n#set up figure\nfig = plt.figure(figsize=(8,6), dpi=600)\nax = plt.axes()\nax.tick_params(axis='x', labelsize=18)\nax.tick_params(axis='y', labelsize=18)\nax.locator_params(nbins=5)\nax.xaxis.set_minor_locator(AutoMinorLocator())\nax.yaxis.set_minor_locator(AutoMinorLocator())\nax.set_xlim(0.0,1.0)\nax.set_ylim(0.0,1.0)\nax.set_yticks([0.1, 0.3, 0.5, 0.7, 0.9])\n\n#labels and true model\nax.set_xlabel('g', fontsize=22)\nax.set_ylabel(r'$\\hat{w}_{k}$', fontsize=22)\n\nax.plot(g, vargp1, 'r', linewidth=3, label=r'$v_{s}^{-1}$')\nax.plot(g, vargp2, 'b', linewidth=3, label=r'$v_{l}^{-1}$')\nax.plot(g, vargp3, 'g', linewidth=3, label=r'$v_{GP}^{-1}$')\n\nax.legend(fontsize=18, loc='upper right')\nplt.show()\n</pre> #pull weights out of fdagger() vargp1 = obj1.var_weights[0] vargp2 = obj1.var_weights[1] vargp3 = obj1.var_weights[2]  #set up figure fig = plt.figure(figsize=(8,6), dpi=600) ax = plt.axes() ax.tick_params(axis='x', labelsize=18) ax.tick_params(axis='y', labelsize=18) ax.locator_params(nbins=5) ax.xaxis.set_minor_locator(AutoMinorLocator()) ax.yaxis.set_minor_locator(AutoMinorLocator()) ax.set_xlim(0.0,1.0) ax.set_ylim(0.0,1.0) ax.set_yticks([0.1, 0.3, 0.5, 0.7, 0.9])  #labels and true model ax.set_xlabel('g', fontsize=22) ax.set_ylabel(r'$\\hat{w}_{k}$', fontsize=22)  ax.plot(g, vargp1, 'r', linewidth=3, label=r'$v_{s}^{-1}$') ax.plot(g, vargp2, 'b', linewidth=3, label=r'$v_{l}^{-1}$') ax.plot(g, vargp3, 'g', linewidth=3, label=r'$v_{GP}^{-1}$')  ax.legend(fontsize=18, loc='upper right') plt.show() <p>This is quite an interesting shape; you should see that the GP (the green curve) takes over in the gap in the mixing process, but then drops in favour of the large-$g$ expansion (blue curve) at the edge of the gap, but then comes back as the dominant model after this for a little bit before mostly surrendering to the large-$g$ expansion. It appears that the jumps in the weights are at the values of $g$ that are close to the training points.</p> <p>In case you wish to compare these results to those of Honda in the paper, where he used the Fractional Power of Rational function (FPR) method (see Sec. 2.3 of this paper) to approximate the true model, you can pull some of his results from the FPR() class and overlay them with the mixed model result we just obtained above.</p> In\u00a0[26]: Copied! <pre>#call the fpr_plot function from FPR() class\nfpr_obj1 = FPR(g, ns, nl)\nfpr_obj1.fpr_plot(mixed_mean, mixed_intervals, fpr_keys=['(3,3)^(1/6)'], ci=68)\n</pre> #call the fpr_plot function from FPR() class fpr_obj1 = FPR(g, ns, nl) fpr_obj1.fpr_plot(mixed_mean, mixed_intervals, fpr_keys=['(3,3)^(1/6)'], ci=68) <p>You can see, in the inset plot, the purple dashed line there, which represents the FPR result. You can change the value of $\\alpha$ to any of the other available values (for $N_{s}, N_{l}=3$, these are 1/2, 1/6, and 1/14) in the fprset() function in the FPR() class, and see how the FPR result changes in comparison to both the true curve and our mixed model result. You see that the FPR result has no uncertainty quantification despite often being rather good at predicting the true curve, making the need for our mixed model clear.</p> <p>Now let's repeat the process of mixing our models for $N_{s} = N_{l} = 2$ and $N_{s} = N_{l} = 5$, as in Fig. 7 in the paper.</p> <p>*Expected Gaussian parameters for for $N_{s}=N_{l}=2$, Matern 3/2 kernel, method 2 below:</p> <p>length_scale = 2.19</p> <p>variance = 2.44</p> <p>Expected values for mean2, sig2, cov2 below:</p> <p>mean2 = [2.55722619 2.55126655 2.54525169 2.53918126 2.53305492...1.57512836 1.57209261 1.56908678 1.56611116 1.56316604]</p> <p>sig2 = [0.01876248 0.01712592 0.01552338 0.01395631 0.01242632 ... 0.0099753  0.00865197 0.00746104 0.00652541 0.00602179]</p> <p>cov2 = [[3.52030736e-04 3.20925503e-04 2.89789816e-04 ... 3.72260454e-07 2.40822250e-07 1.10005451e-07] [3.20925503e-04 2.93297249e-04 2.65481431e-04 ... 3.46610444e-07 2.24230144e-07 1.02428391e-07] ... [2.40822250e-07 2.24230144e-07 2.07502354e-07 ... 4.73465254e-05 4.25809124e-05 3.75668099e-05] [1.10005451e-07 1.02428391e-07 9.47893683e-08 ... 3.87738358e-05 3.75668099e-05 3.62619107e-05]]</p> <p>Expected values for mixed_mean2, mixed_intervals2 below:</p> <p>mixed_mean2 = [2.50662827 2.50643831 2.50586856 2.50491903 2.50358975 ... 1.5751143  1.57208418 1.56908052 1.56610477 1.56315758]</p> <p>mixed_intervals2 = [[2.50662827 2.50662827] [2.50643825 2.50643836] [2.50586769 2.50586943]...[1.56230621 1.57585484] [1.56006899 1.57214055] [1.55753961 1.56877554]]</p> In\u00a0[27]: Copied! <pre>#new object for N_s = N_l = 2 and same steps as before\nns = 2\nnl = 2\nobj2 = GP(g, ns, nl, ci=68, kernel=\"Matern\", nu=1.5, error_model='informative')\nobj_tr2 = obj2.training(plot=False)\nmean2, sig2, cov2 = obj2.validate(plot=True)\nmixed_mean2, mixed_intervals2 = obj2.plot_mix(g, plot_fdagger=True, plot_true=True, GP_mean=mean2, GP_var=np.square(sig2))\n</pre> #new object for N_s = N_l = 2 and same steps as before ns = 2 nl = 2 obj2 = GP(g, ns, nl, ci=68, kernel=\"Matern\", nu=1.5, error_model='informative') obj_tr2 = obj2.training(plot=False) mean2, sig2, cov2 = obj2.validate(plot=True) mixed_mean2, mixed_intervals2 = obj2.plot_mix(g, plot_fdagger=True, plot_true=True, GP_mean=mean2, GP_var=np.square(sig2)) <pre>Gaussian process parameters: 2.53**2 * Matern(length_scale=2.34, nu=1.5)\n</pre> <p>Expected Gaussian parameters for $N_{s}=N_{l}=5$, Matern 3/2 kernel, method 2 below:</p> <p>length_scale = 0.889</p> <p>variance = 1.43</p> <p>Expected values for mean3, sig3, cov3 below:</p> <p>mean3 = [2.51973149 2.51700401 2.51418648 2.51127741 2.50827525...1.55654875 1.55573191 1.55497964 1.55429276 1.55367207]</p> <p>sig3 = [0.03393844 0.03096305 0.02803949 0.02517217 0.02236587... 0.0198764  0.01557175 0.01120256 0.00676925 0.00227229]</p> <p>cov3 = [[ 1.15181746e-03  1.04912112e-03  9.45330624e-04 ... -1.83742307e-06 -1.09250665e-06 -3.60753668e-07] [ 1.04912112e-03  9.58710327e-04  8.66592363e-04 ... -1.71804751e-06 -1.02152758e-06 -3.37315859e-07] ... [-1.09250665e-06 -1.02152758e-06 -9.49045906e-07 ...  7.55800038e-05 4.58227379e-05  1.52962535e-05] [-3.60753668e-07 -3.37315859e-07 -3.13381883e-07 ...  2.51179533e-05 1.52962535e-05  5.16330684e-06]]</p> <p>Expected values for mixed_mean3, mixed_intervals3 below:</p> <p>mixed_mean3 = [2.50662827 2.50643839 2.5058699  2.50492583 2.5036112 ... 1.56617842 1.56330951 1.56045572 1.55761692 1.55479279]</p> <p>mixed_intervals3 = [[2.50662827 2.50662827] [2.50643839 2.50643839] [2.5058699  2.50586991]...[1.56040869 1.56050275] [1.55757141 1.55766243] [1.55474875 1.55483682]]</p> In\u00a0[28]: Copied! <pre>#new object for N_s = N_l = 5 and same steps as before\nns = 5\nnl = 5\nobj3 = GP(g, ns, nl, ci=68, kernel=\"Matern\", nu=1.5, error_model='informative')\nobj_tr3 = obj3.training(plot=False)\nmean3, sig3, cov3 = obj3.validate(plot=True)\nmixed_mean3, mixed_intervals3 = obj3.plot_mix(g, plot_fdagger=True, plot_true=True, GP_mean=mean3, GP_var=np.square(sig3))\n</pre> #new object for N_s = N_l = 5 and same steps as before ns = 5 nl = 5 obj3 = GP(g, ns, nl, ci=68, kernel=\"Matern\", nu=1.5, error_model='informative') obj_tr3 = obj3.training(plot=False) mean3, sig3, cov3 = obj3.validate(plot=True) mixed_mean3, mixed_intervals3 = obj3.plot_mix(g, plot_fdagger=True, plot_true=True, GP_mean=mean3, GP_var=np.square(sig3)) <pre>Gaussian process parameters: 1.42**2 * Matern(length_scale=0.865, nu=1.5)\n</pre> <p>These results also look pretty good! We can say that this method is by far the best for our toy model, and we cannot wait to apply this to real nuclear physics applications! Stay tuned for the next release, where you will be able to input your own functions and test them in this sandbox!</p> <p>Written by: Alexandra Semposki (07 June 2022)</p>"},{"location":"Tutorials/GP_BMM/#an-introduction-to-samba-trivariate-bayesian-model-mixing-wgps","title":"An introduction to SAMBA: Trivariate Bayesian Model Mixing w/GPs\u00b6","text":"<p>Alexandra Semposki</p> <p>Date: 07 June 2022</p>"},{"location":"Tutorials/GP_BMM/#introduction","title":"Introduction\u00b6","text":""},{"location":"Tutorials/GP_BMM/#setting-up-a-gaussian-process-model","title":"Setting up a Gaussian Process model\u00b6","text":""},{"location":"Tutorials/GP_BMM/#multivariate-model-mixing-ppd","title":"Multivariate model mixing: PPD\u00b6","text":"<p>Now comes the time to mix this model in with the two expansions and see what we get. To do this, we call plot_mix() from the Bivariate() class again, but this time we send it the GP mean and variance results.</p>"},{"location":"Tutorials/GP_BMM/#diagnostics-mahalanobis-distance","title":"Diagnostics: Mahalanobis distance\u00b6","text":""},{"location":"Tutorials/GP_BMM/#weights","title":"Weights\u00b6","text":"<p>Now let's look at what the weights of each function look like (the normalized precisions of each model) across the input space, $g$. The actual weight values are contained within a class variable in the fdagger() function, under the name var_weights.</p>"},{"location":"Tutorials/GP_BMM/#fpr-comparison","title":"FPR Comparison\u00b6","text":""},{"location":"Tutorials/LMM/","title":"An introduction to SAMBA: Linear Mixture Model","text":"<p>Welcome to SAMBA, our toddler computational package that allows you to explore the world of Bayesian model mixing! SAMBA (SAndbox for Mixing using Bayesian Analysis) currently supports three methods of model mixing developed in our paper (see this arXiv link). In the future, we will release a version of this package where you, the reader, can input your own functions to mix. At the time of this release, however, we've got our toy problem set up for you to play with.</p> <p>Let's quickly define the toy problem from the paper linked above. We want to mix the expansions of the zero-dimensional $\\phi^4$-theory partition function, below:</p> <p>$$  F(g) = \\int_{-\\infty}^{\\infty} dx~ e^{-\\frac{x^{2}}{2} - g^{2} x^{4}} = \\frac{e^{\\frac{1}{32 g^{2}}}}{2 \\sqrt{2}g} K_{\\frac{1}{4}}\\left(\\frac{1}{32 g^{2}} \\right). $$</p> <p>The two expansions are limits taken at $g = 0$ and $g = \\infty$:</p> <p>$$ F_{s}^{N_s}(g) = \\sum_{k=0}^{N_{s}} s_{k} g^{k}, $$</p> <p>and</p> <p>$$ F_{l}^{N_{l}}(g) = \\frac{1}{\\sqrt{g}} \\sum_{k=0}^{N_{l}} l_{k} g^{-k}, $$</p> <p>with coefficients given as:</p> <p>$$ s_{2k} = \\frac{\\sqrt{2} \\Gamma{(2k + 1/2)}}{k!} (-4)^{k},~~~~~s_{2k + 1} = 0 $$</p> <p>and</p> <p>$$ l_{k} = \\frac{\\Gamma{\\left(\\frac{k}{2} + \\frac{1}{4}\\right)}}{2k!} \\left(-\\frac{1}{2}\\right)^{k}. $$</p> <p>We begin by importing all of the Python packages we will need in this Jupyter notebook.</p> In\u00a0[2]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport math\nimport statistics\nimport emcee\nimport corner\nfrom cycler import cycler\nfrom scipy import stats, special, integrate\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import AutoMinorLocator\n%matplotlib inline\n\n#matplotlib settings for Latex plots\nimport matplotlib\n</pre> %load_ext autoreload %autoreload 2  import numpy as np import math import statistics import emcee import corner from cycler import cycler from scipy import stats, special, integrate  import matplotlib.pyplot as plt from matplotlib.ticker import AutoMinorLocator %matplotlib inline  #matplotlib settings for Latex plots import matplotlib <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> <p>Now we import the classes from SAMBA that we'll need for the Linear Mixture Model (LMM from here on).</p> In\u00a0[3]: Copied! <pre>import sys\nsys.path.append('../../')\n\nfrom samba.models import Models, Uncertainties\nfrom samba.mixing import LMM\nfrom samba.priors import Priors\n</pre> import sys sys.path.append('../../')  from samba.models import Models, Uncertainties from samba.mixing import LMM from samba.priors import Priors <p>The first two classes, Models and Uncertainties, give our toy model and its theory errors. LMM is the class that performs the linear mixture method, and Priors are the priors needed to sample any parameters in this model. This will be clear later on.</p> <p>Now we plot the functions we want to mix to see what they look like. We need to first pick a value for $N_{s}$ and $N_{l}$. Let's pick $N_{s}$ = 2 and $N_{l}$ = 2, as in the paper.</p> In\u00a0[5]: Copied! <pre>#set up the linspace for input variable g and N_s and N_l\ng = np.linspace(1e-6,1.0,100)\nns = np.array([2])\nnl = np.array([2])\n\n#call the plot_models() function from Models()\nm = LMM(ns, nl, error_model='informative')\nplot = m.plot_models(g)\n</pre> #set up the linspace for input variable g and N_s and N_l g = np.linspace(1e-6,1.0,100) ns = np.array([2]) nl = np.array([2])  #call the plot_models() function from Models() m = LMM(ns, nl, error_model='informative') plot = m.plot_models(g) <p>Now let's try to mix these two functions using the LMM() class. We'll need data in the gap, as this method relies on some data there. To do this, we estimate the length of the gap by eye and make a linspace over it with a few data points.</p> In\u00a0[6]: Copied! <pre>#make linspace for data\ng_data = np.linspace(0.1, 0.6, 10)\n\n#call the add_data() function to create the data\ndata, sigma = m.add_data(g, g_data, error=0.01)\n</pre> #make linspace for data g_data = np.linspace(0.1, 0.6, 10)  #call the add_data() function to create the data data, sigma = m.add_data(g, g_data, error=0.01) <p>Now we call the mixing function, mixed_model(), and give it this data. It will ask us which mixing function we wish to use, out of three options: logistic, cdf, and piecewise cosine. As discussed in the paper, the best option is the piecewise cosine. This mixing function will require emcee to sample 3 parameters, as we'll see later on in a plot we can generate with the results of the sampling.</p> <p>The equation the sampler will solve is the posterior written as $$ p(\\theta|\\mathbf{D}) = p(\\theta)\\prod_{i=1}^{n} \\left\\{ \\alpha(g_{i}; \\theta) ~\\mathcal{N}(F^{N_s}_{s}(g_{i}), \\sigma_{d_{i}}^{2} + \\sigma_{N_s}^2) + (1 - \\alpha(g_{i}; \\theta))~ \\mathcal{N}(F^{N_l}_{l}(g_{i}), \\sigma_{d_{i}}^{2} + \\sigma_{N_l}^2) \\right\\}, $$</p> <p>where $\\alpha(g;\\theta)$ is this piecewise cosine mixing function (see Eq. (15) in the paper), the two individual models are written as Gaussian distributions, and their variances, $\\sigma_{d_{i}}^{2} + \\sigma_{N_s}^2$ and $\\sigma_{d_{i}}^{2} + \\sigma_{N_l}^2$, are combinations of the error on the data points ($\\sigma_{d_{i}}^{2}$) and the theory error at each data point ($\\sigma_{N}^2$). The prior $p(\\theta)$ is given as three truncated Gaussians in the form</p> <p>$$ p(\\theta) = \\mathcal{U}(\\theta_{1} \\in [0,b])~\\mathcal{N}(\\theta_{1}; \\mu_{1}, 0.05)         \\times\\mathcal{U}(\\theta_{2} \\in [\\theta_{1},b])~\\mathcal{N}(\\theta_{2}; \\mu_{2}, 0.05)         \\times\\mathcal{U}(\\theta_{3} \\in [\\theta_{2},b])~\\mathcal{N}(\\theta_{3}; \\mu_{3}, 0.05). $$</p> <p>The means $\\mu$ and ranges $b$, as well as the variance (given there as 0.05 for each prior) are dependent upon the series expansion chosen and therefore can be altered as need arises.</p> <p>NOTE: If you merely choose some data points and run the code, you will notice that, when all is said and done, the parameters may not be located in very accurate spots in the gap. This could be because the Priors() function lpdf() needs some adjusting, by you! Your freedom in the toy model so far is to adjust the information in the Priors() class so that the best starting points for the sampler are given to it. As mentioned above, the mean and variance of the Gaussian prior can be changed to better match where one would expect that parameter to be located, and the uniform prior helps the sampler keep the parameters from crossing when they are sampled. They must not cross, else the sampler will not be happy and the mixing will not work properly. Hence, adjusting the uniform prior so that the parameters remain in their own regions, but allowing them some wiggle room via the Gaussian variance, is the best method when dealing with the piecewise cosine mixing function.</p> <p>So far, the best way change these parameters is by directly altering the class file priors.py yourself, but in the future when more flexibility for users is added, lpdf() will become a function you can directly input into the class without changing anything internally in the package.</p> <p>Note that the values here will not be absolute---the emcee sampler will allow for some differences between runs, as it is stochastic, and the data generated in this notebook is randomized for each run. However, expected values for the trace and onward in this notebook should be within a few percent of the results you obtain yourself using the prior values below:</p> <p>Prior information for benchmark (set within priors.py file):</p> <p>g1 = self.luniform(params[0], 0.01, 0.3) + stats.norm.logpdf(params[0], 0.1, 0.05)</p> <p>g3 = self.luniform(params[2], params[0], 0.55) + stats.norm.logpdf(params[2], 0.4, 0.05)</p> <p>g2 = self.luniform(params[1], params[2], 0.8) + stats.norm.logpdf(params[1], 0.6, 0.05)</p> <p>*Expected values of trace for $N_{s}=N_{l}=2$, cosine mixing function, nsteps=3000:</p> <p>trace = [[0.07418137 0.07418137 0.03490732 ... 0.12225827 0.14157856 0.16379292] [0.59066046 0.59066046 0.54589913 ... 0.51536977 0.50214119 0.48472922] [0.39022897 0.39022897 0.37559441 ... 0.41987635 0.4359556  0.42130717]]</p> In\u00a0[7]: Copied! <pre>#call mixed_model()\nchain, trace = m.mixed_model(g_data, data, sigma, mixing_function='cosine', nsteps=3000)\n</pre> #call mixed_model() chain, trace = m.mixed_model(g_data, data, sigma, mixing_function='cosine', nsteps=3000) <pre>Using 20 walkers with 3000 steps each, for a total of 60000 samples.\nCalculation finished!\nDuration = 1 min, 33 sec.\n</pre> <p>We have the samples in hand now. However, we would like to thin them so that we cut out any correlations in the samples. This allows us to capture the most important features of the samples. Let's do this using the autocorrelation length in the function stats_chain(). The results for the mean and median of the mixing function parameter distributions will also be returned from this function, along with the thinned array for any further work we wish to do with the sample results.</p> <p>Expected values for thin_array, mean, median for $N_{s}=N_{l}=2$, cosine mixing function:</p> <p>thin_array = [[0.13377046 0.15029928 0.2248071  ... 0.17242319 0.19149209 0.11299201] [0.31068    0.30495626 0.49641581 ... 0.51533557 0.63964489 0.57477458] [0.21107303 0.25329124 0.37645051 ... 0.36013452 0.36440693 0.42444696]]</p> <p>mean = [0.11096199 0.5882248  0.38049597]</p> <p>median = [0.10839164 0.59186747 0.38149871]</p> In\u00a0[8]: Copied! <pre>thin_array, mean, median = m.stats_chain(chain)\n</pre> thin_array, mean, median = m.stats_chain(chain) <p>Now that we know these values, we can take a look at the mixing function we used (before looking at the final PPD result of the mixed model). To do this properly, we employ the MAP value results from the function MAP_values(), instead of using the mean or the median, as these can be misleading when one's distributions are not purely Gaussian. We then overlay the MAP values with the mixing curve itself, to show where the parameter values have landed in $g$.</p> <p>Expected MAP values for $N_{2}=N_{l}=2$, cosine mixing function:</p> <p>map_values = [0.10497989 0.59295989 0.38282007]</p> In\u00a0[9]: Copied! <pre>map_values = m.MAP_values(thin_array, g, g_data, data, sigma)\n\nprint(map_values)\n</pre> map_values = m.MAP_values(thin_array, g, g_data, data, sigma)  print(map_values) <pre>[0.10164784 0.59502968 0.3833919 ]\n</pre> In\u00a0[10]: Copied! <pre>#define a weight plot function for simplicity\ndef weight_plot(g, map_values):\n    \n    #set up figure\n    fig = plt.figure(figsize=(8,6), dpi=600)\n    ax = plt.axes()\n    ax.tick_params(axis='x', labelsize=18)\n    ax.tick_params(axis='y', labelsize=18)\n    ax.locator_params(nbins=5)\n    ax.xaxis.set_minor_locator(AutoMinorLocator())\n    ax.yaxis.set_minor_locator(AutoMinorLocator())\n    ax.set_xlim(0.0,1.0)\n    ax.set_ylim(0.0,1.0)\n    ax.set_yticks([0.1, 0.3, 0.5, 0.7, 0.9])\n\n    #labels and true model\n  #  ax.set_title('Method: {}'.format(k))\n    ax.set_xlabel('g', fontsize=22)\n    ax.set_ylabel(r'$\\hat{w}_{k}$', fontsize=22)\n    \n    #calculate the cosine function\n    cosine = np.zeros(len(g))\n    for i in range(len(g)):\n        cosine[i] = m.switchcos(map_values, g[i])\n\n    #plot the weights\n    ax.plot(g, cosine, 'r', linewidth=3, label=r'$\\alpha(g;\\theta)$')\n    ax.plot(g, 1-cosine, 'b', linewidth=3, label=r'$1 - \\alpha(g;\\theta)$')\n    \n    #plot the training points as lines\n    ax.axvline(x=map_values[0], color='darkorange', linestyle='dashed', label=r'$\\theta_1$')\n    ax.axvline(x=map_values[2], color='darkviolet', linestyle='dashdot', label=r'$\\theta_2$')\n    ax.axvline(x=map_values[1], color='deeppink', linestyle='dashed', label=r'$\\theta_3$')\n\n    ax.legend(fontsize=14, loc='upper right', frameon=False)\n    plt.show()\n    \n    return None \n</pre> #define a weight plot function for simplicity def weight_plot(g, map_values):          #set up figure     fig = plt.figure(figsize=(8,6), dpi=600)     ax = plt.axes()     ax.tick_params(axis='x', labelsize=18)     ax.tick_params(axis='y', labelsize=18)     ax.locator_params(nbins=5)     ax.xaxis.set_minor_locator(AutoMinorLocator())     ax.yaxis.set_minor_locator(AutoMinorLocator())     ax.set_xlim(0.0,1.0)     ax.set_ylim(0.0,1.0)     ax.set_yticks([0.1, 0.3, 0.5, 0.7, 0.9])      #labels and true model   #  ax.set_title('Method: {}'.format(k))     ax.set_xlabel('g', fontsize=22)     ax.set_ylabel(r'$\\hat{w}_{k}$', fontsize=22)          #calculate the cosine function     cosine = np.zeros(len(g))     for i in range(len(g)):         cosine[i] = m.switchcos(map_values, g[i])      #plot the weights     ax.plot(g, cosine, 'r', linewidth=3, label=r'$\\alpha(g;\\theta)$')     ax.plot(g, 1-cosine, 'b', linewidth=3, label=r'$1 - \\alpha(g;\\theta)$')          #plot the training points as lines     ax.axvline(x=map_values[0], color='darkorange', linestyle='dashed', label=r'$\\theta_1$')     ax.axvline(x=map_values[2], color='darkviolet', linestyle='dashdot', label=r'$\\theta_2$')     ax.axvline(x=map_values[1], color='deeppink', linestyle='dashed', label=r'$\\theta_3$')      ax.legend(fontsize=14, loc='upper right', frameon=False)     plt.show()          return None  In\u00a0[12]: Copied! <pre>weight_plot(g, map_values)\n</pre> weight_plot(g, map_values) <p>Now we want to calculate the mixed model, which means we need to obtain the posterior predictive distribution, or PPD, as a function of points in $g$ that can be spread across the input space. For this, we make another array in $g$ and call the ppd() function, employing our thinned trace as well.</p> <p>The PPD equation that is solved in this code is given as</p> <p>$$ p(\\tilde y(g)|\\theta, \\mathbf{D}) = \\sum_{j=1}^{M} \\alpha(g; \\theta_{j}) ~\\mathcal{N}(F^{N_s}_{s}(g), \\sigma_{d_{i}}^{2} + \\sigma_{N_s}^2) + (1 - \\alpha(g; \\theta_{j}))~ \\mathcal{N}(F^{N_l}_{l}(g), \\sigma_{d_{i}}^{2} + \\sigma_{N_l}^2), $$</p> <p>much like before with the sampled posterior, except now we are predicting for a new observation $\\tilde y(g)$.</p> <p>Expected median_results, intervals for $N_{s}=N_{l}=2$, cosine mixing function:</p> <p>median_results = [2.50662827, 2.50643831, 2.50586856, 2.50491903, 2.50358972...1.57507493, 1.57205394, 1.56905142, 1.56606715, 1.56310093] intervals = [[2.50662827, 2.50662827], [2.50643831, 2.50643831], [2.50586856, 2.50586856],...[1.56905142, 1.56905142], [1.56606715, 1.56606715], [1.56310093, 1.56310093]]</p> In\u00a0[13]: Copied! <pre>#PPD linspace\ng_ppd = np.linspace(1e-6, 1.0, 200)\n\n#PPD calculation using ppd() and MAP parameter values\nmedian_results, intervals = m.ppd(thin_array, map_values, g_data, g_ppd, data, 0.68)\n</pre> #PPD linspace g_ppd = np.linspace(1e-6, 1.0, 200)  #PPD calculation using ppd() and MAP parameter values median_results, intervals = m.ppd(thin_array, map_values, g_data, g_ppd, data, 0.68) <p>And that's the mixed model result! The green curve is our PPD result, which has been generated using the median of the posterior at each point in $g$. The green band is the credibility interval at 68%, using the HPD (Highest Posterior Density) method, which is contained in the function hpd_interval() in the LMM() class, and does its work by finding the smallest region with 68% of the posterior density and plotting it.</p> <p>It is quite evident, however, that the mixed model here is not matching the true curve very well at all. In the next notebook (Bivariate_BMM) we will look at another method to mix these two series expansions together.</p> <p>Written by: Alexandra Semposki (06 June 2022)</p>"},{"location":"Tutorials/LMM/#an-introduction-to-samba-linear-mixture-model","title":"An introduction to SAMBA: Linear Mixture Model\u00b6","text":"<p>Alexandra Semposki</p> <p>Date: 06 June 2022</p>"},{"location":"Tutorials/LMM/#introduction-and-model-setup","title":"Introduction and model setup\u00b6","text":""},{"location":"Tutorials/LMM/#bmm-using-the-linear-model-mixing-method","title":"BMM using the Linear Model Mixing method\u00b6","text":""},{"location":"Tutorials/LMM/#adding-data","title":"Adding data\u00b6","text":""},{"location":"Tutorials/LMM/#choosing-a-function-for-the-weights-and-parameter-estimation-using-mcmc","title":"Choosing a function for the weights and parameter estimation using MCMC\u00b6","text":""},{"location":"Tutorials/LMM/#weights","title":"Weights\u00b6","text":""},{"location":"Tutorials/LMM/#calculating-the-posterior-predictive-distribution-ppd","title":"Calculating the posterior predictive distribution (PPD)\u00b6","text":""}]}